---
title: "PML: Weight Lifting Exercise"
output: html_document
---
###Background

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har. 

Our goal is to predict in which one of the 5 ways that participants performed in 20 separate qualitative measurements.


###Data 


The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

###Load required libraries
```{r}
library(caret)
library(randomForest)
library(rattle)
library(sqldf)
library(doBy)

```

###Read data

```{r}
pmltraining <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
pmltesting <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
#pmltraining1 <- read.csv("/media/S/Temp/pml-training.csv")
#pmltesting1 <- read.csv("/media/S/Temp/pml-testing.csv")

```

###Data cleaning and partition for validation
```{r}
dim(pmltraining);dim(pmltesting)
```

We check the data quality in training dataset before modeling. We first convert `#DIV/0!` in a numeric field to `NA`, then use function `colSums` to identify columns that contains missing values. 

These columns together with a number of timestamp columns are removed from the training dataset. The same cleaning rule is then applied to the testing dataset.

(We also tried to remove missing records by rows but it turned out to be inapplicable because 98% of rows contain at least one missing value in a certain field.)

We also separated the testing dataset into testing and validating partitions. Set seed so the same result can be reproduced.
```{r}
# To see the missing values
# summary(pmltraining)

# use training data only to remove NA columns
prePro<-pmltraining
prePro[prePro=='#DIV/0!']<-NA
nona<-colSums(is.na(prePro)) == 0
nona[c(1,3:5)]<-FALSE

set.seed(1234)
mlData<-pmltraining[,nona]
inTrain<-createDataPartition(y=mlData$classe,p=0.5,list=F)
training<-mlData[inTrain,]
validating<-mlData[-inTrain,]

#do the same variable selection on testing dataset
testing<-pmltesting[,nona]
```

###Exploratory plots

Don't know what plots are appropriate yet ...
```{r echo=FALSE}
toPlot<-sqldf('select
              classe,
              total_accel_belt,
              total_accel_arm,
              total_accel_dumbbell,
              total_accel_forearm
              from training ')
featurePlot(toPlot[,2:5],toPlot[,1],plot='pairs',xlab='total accel')

qplot(total_accel_belt,total_accel_arm,col=classe,data=training,
      main='Predictors by classe')
```

###Data preprocessing

Tried PCA but it didn't help - will drop from the writeup
```{r echo=FALSE}
# to see the predictors
# names(training)
lastCol<-ncol(training)
preProc<-preProcess(training[,-c(1:6,lastCol)],method='pca',thresh=.995)
trainPC<-predict(preProc,training[,-c(1:6,lastCol)])
validPC<-predict(preProc,validating[,-c(1:6,lastCol)])
testPC<-predict(preProc,testing[,-c(1:6,lastCol)])
system.time(PCFit<-randomForest(training$classe~.,data=trainPC))
PCFit

```


###Modeling
```{r}
# very slow!
# system.time(modFit<-train(classe~.,method='rf',data=trainPC,prox=T))
# modFit$finalModel

system.time(modFit<-randomForest(classe~.,data=training)) #reduce ntree=200 when it is slow
modFit

```
We have a pretty decent model from our training data. The out of box estimate of __error rate is 0.6%__.

On the other hand, PC fit is not as good as the fit from all training data, even we kept 99.5% threshood in the PCA model.

To estimate out of sample error, we use the validation data for cross-validation.

###Cross-Validation
```{r}
pred<-predict(modFit,validating)
a<-table(pred,validating$classe)
oos<-1-sum(diag(a))/sum(a)
a
```

The estimated __out of sample error is `r round(1-oos,4)*100`%__.

###Testing
We now come to testing stage. We need to predict the outcomes from the 20 testing data records labeled by `problem_id_1` to `problem_id_20` using the random forest model we just built.

Notice that the variable `new_window` in testing dataset doesn't have the same levels as that in the training data. We have to add a new level `yes` into it before it can be used in the model fit from the training model.

```{r}
levels(testing$new_window)<-c(levels(testing$new_window),'yes')
answers<-predict(modFit,testing)
```

[Here is the GitHub page.](http://lisun7170.github.io/pml/coursera_wleproj.html)

